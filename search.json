[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning 2022",
    "section": "",
    "text": "Registrar avances generales del curso de Machine Learning en Data Science"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introducci√≥n",
    "section": "",
    "text": "Identificar elementos clave de ciencia de datos\nComprender c√≥mo interact√∫an los elementos algor√≠tmicos para afectar el rendimiento\nComprender c√≥mo elegir algoritmos para diferentes tareas de an√°lisis\nAnalizar los datos de una manera exploratoria y espec√≠fica\nImplementar y aplicar algoritmos b√°sicos para el aprendizaje supervisado y sin supervisi√≥n\nEvaluar con precisi√≥n el rendimiento de los algoritmos."
  },
  {
    "objectID": "intro.html#qu√©-es-data-science",
    "href": "intro.html#qu√©-es-data-science",
    "title": "Introducci√≥n",
    "section": "¬øQu√© es Data Science?",
    "text": "¬øQu√© es Data Science?\n\n\n\nData Science\n\n\n\nAnalytics:\n\nUso de datos y m√©todos cuantitativos para mejorar la toma de decisiones"
  },
  {
    "objectID": "intro.html#niveles-de-informaci√≥n",
    "href": "intro.html#niveles-de-informaci√≥n",
    "title": "Introducci√≥n",
    "section": "Niveles de Informaci√≥n",
    "text": "Niveles de Informaci√≥n\n\n\n\nNiveles Informaci√≥n"
  },
  {
    "objectID": "intro.html#flujos-de-informaci√≥n",
    "href": "intro.html#flujos-de-informaci√≥n",
    "title": "Introducci√≥n",
    "section": "Flujos de Informaci√≥n",
    "text": "Flujos de Informaci√≥n\n\n\n\nFlujos de Informaci√≥n"
  },
  {
    "objectID": "intro.html#niveles-de-ia",
    "href": "intro.html#niveles-de-ia",
    "title": "Introducci√≥n",
    "section": "Niveles de IA",
    "text": "Niveles de IA\n\nSistemas autom√°ticos\nRedes Neuronales Artificiales\nRob√≥tica Cognitiva"
  },
  {
    "objectID": "intro.html#tipos-de-aprendizaje-autom√°tico",
    "href": "intro.html#tipos-de-aprendizaje-autom√°tico",
    "title": "Introducci√≥n",
    "section": "Tipos de aprendizaje autom√°tico",
    "text": "Tipos de aprendizaje autom√°tico\n\n\n\nTipos de ML"
  },
  {
    "objectID": "intro.html#esquema-conceptual",
    "href": "intro.html#esquema-conceptual",
    "title": "Introducci√≥n",
    "section": "Esquema conceptual",
    "text": "Esquema conceptual\n\n\n\nConceptos en Machine Learning"
  },
  {
    "objectID": "C2-Features_engineering.html",
    "href": "C2-Features_engineering.html",
    "title": "2¬† Feature Engineering",
    "section": "",
    "text": "Una cantidad correcta de atributos ayudan a crear mejores modelos.\nLos datos de altas dimensiones se vuelven cada vez m√°s dispersos en su espacio.\nLas definiciones de densidad y distancia entre puntos se vuelven menos significativas a mayor numero de atributos.\n\n\n\n\nLa Maldici√≥n de la dimensionalidad"
  },
  {
    "objectID": "C2-Features_engineering.html#an√°lisis-exploratorio-de-datos-eda",
    "href": "C2-Features_engineering.html#an√°lisis-exploratorio-de-datos-eda",
    "title": "2¬† Feature Engineering",
    "section": "2.2 An√°lisis exploratorio de datos (EDA)",
    "text": "2.2 An√°lisis exploratorio de datos (EDA)\nAn√°lisis del conjunto de datos para resumir sus principales caracter√≠sticas, mediante m√©todos estad√≠sticos y visuales.\n\n2.2.1 Objetivos EDA:\n\nDescubrir la estructura subyacente de los datos\nIdentificar variables relevantes Detectar valores at√≠picos y anomal√≠as Validar supuestos\nGenerar hip√≥tesis a partir de los datos\n\n\n\n\nEjemplo de An√°lisis exploratorio de datos (EDA)"
  },
  {
    "objectID": "C2-Features_engineering.html#definici√≥n-de-ingenier√≠a-de-atributos",
    "href": "C2-Features_engineering.html#definici√≥n-de-ingenier√≠a-de-atributos",
    "title": "2¬† Feature Engineering",
    "section": "2.3 Definici√≥n de Ingenier√≠a de Atributos",
    "text": "2.3 Definici√≥n de Ingenier√≠a de Atributos\n\nSelecci√≥n de atributos:\n\nSelecci√≥n de un subconjunto de atributos seg√∫n alg√∫n criterio espec√≠fico.\n\n\n\n\n\nSelecci√≥n de atributos\n\n\n\nExtracci√≥n de atributos:\n\nCreaci√≥n de nuevos atributos a partir de atributos originales\n\n\n\n\n\nExtracci√≥n de atributos\n\n\nPueden hacerse con conocimiento del dominio o algor√≠tmicamente"
  },
  {
    "objectID": "C2-Features_engineering.html#objetivos-de-feature-engineering",
    "href": "C2-Features_engineering.html#objetivos-de-feature-engineering",
    "title": "2¬† Feature Engineering",
    "section": "2.4 Objetivos de Feature Engineering",
    "text": "2.4 Objetivos de Feature Engineering\n\nMejorar el desempe√±o de los modelos:\n\nPoder predictivo\nComplejidad\nTiempo de ejecuci√≥n\n\nVisualizar los datos\nEliminar el ruido"
  },
  {
    "objectID": "C2-Features_engineering.html#sec-distancias",
    "href": "C2-Features_engineering.html#sec-distancias",
    "title": "2¬† Feature Engineering",
    "section": "2.5 Distancias",
    "text": "2.5 Distancias\nUna m√©trica que mide la distancia entre un par de entidades dados los dos puntos x e y, una funci√≥n m√©trica o de distancia debe cumplir las siguientes condiciones:\n\nTabla de Condiciones de Distancias\n\n\nNombre Condici√≥n\nCondici√≥n\n\n\n\n\nNo negatividad\nd(x,y) >0\n\n\nIdentidad\nd(x,y)=0 <=> x=y\n\n\nSimetr√≠a\nd(x,y)=d(y,x)\n\n\nDesigualdad triangular\nd(x,z) <= d(x,y)+d(y,z)\n\n\n\n\n2.5.1 Tipos de Distancias\n\n2.5.1.1 Distancia Euclediana\nSe desprende del teorema de Pit√°goras\n\n\n\nDistancia Euclediana\n\n\nd_E(P_1, P_2)=\\sqrt{(x_2-x_1)^2+(y_2-y_1)^2}\n\n\n2.5.1.2 Distancia de Minkowski\nGeneralizaci√≥n de la distancia euclidiana. Sean que p y q vectores m-dimensionales\n\n\n\nFormulaci√≥n de Distancia de Minkowski\n\n\n\n\n2.5.1.3 Distancia de Mahalanobis\n\n\n\nF√≥rmulaci√≥n de Distancia de Minkowski\n\n\n\n\n2.5.1.4 Distancia del Coseno\n #### Distancia Suave del Coseno\n\n\n\nDistancia Suave del Coseno\n\n\n\n\n\n2.5.2 Matriz de Distancias\n\n\n\nMatriz de Distancias"
  },
  {
    "objectID": "C2-Features_engineering.html#similitud",
    "href": "C2-Features_engineering.html#similitud",
    "title": "2¬† Feature Engineering",
    "section": "2.6 Similitud",
    "text": "2.6 Similitud\nSimilitud:\n\nMide lo parecidos que son dos entidades.\nEs m√°s alto cuando entidades son m√°s parecidas.\n\nDisimilitud\n\nMide cuan diferentes son dos entidades\nM√°s bajo cuando los objetos son m√°s parecidos\n\n\n\n\nTabla de Similitud y Disimilitud"
  },
  {
    "objectID": "C2-Features_engineering.html#referencias",
    "href": "C2-Features_engineering.html#referencias",
    "title": "2¬† Feature Engineering",
    "section": "2.7 Referencias:",
    "text": "2.7 Referencias:\n√çndice de C√°lculo de Distancias"
  },
  {
    "objectID": "C3-clusters.html",
    "href": "C3-clusters.html",
    "title": "3¬† An√°lisis de Cluster",
    "section": "",
    "text": "Clusters"
  },
  {
    "objectID": "C3-clusters.html#objetivos",
    "href": "C3-clusters.html#objetivos",
    "title": "3¬† An√°lisis de Cluster",
    "section": "3.1 Objetivos",
    "text": "3.1 Objetivos\nEncontrar algor√≠tmicamente grupos de entidades tales que:\n\nLa similitud intragrupo es alta\nLa similitud entre grupos es baja\n\nLas medidas de distancia y similitud son cruciales en este proceso"
  },
  {
    "objectID": "C3-clusters.html#criterios-para-agrupar",
    "href": "C3-clusters.html#criterios-para-agrupar",
    "title": "3¬† An√°lisis de Cluster",
    "section": "3.2 Criterios para agrupar",
    "text": "3.2 Criterios para agrupar\nExisten varias nociones de clusters basados en:\nDistancias: Cada punto est√° m√°s cerca de todos los puntos de su grupo que cualquier punto de otro grupo.\nCentroide: Cada punto est√° m√°s cerca del centro de su grupo que del centro de cualquier otro grupo.\nContig√ºidad: Cada punto est√° m√°s cerca de al menos un punto de su grupo que cualquier punto de otro grupo.\nDensidad: Los clusters son regiones de alta densidad separadas por regiones de baja densidad."
  },
  {
    "objectID": "C3-clusters.html#estad√≠stico-de-hopkins",
    "href": "C3-clusters.html#estad√≠stico-de-hopkins",
    "title": "3¬† An√°lisis de Cluster",
    "section": "3.3 Estad√≠stico de Hopkins",
    "text": "3.3 Estad√≠stico de Hopkins\nAntes de agrupar un conjunto de datos, podemos probar si realmente hay cl√∫steres. Necesitamos probar la hip√≥tesis de la existencia de patrones en los datos contra un conjunto de datos distribuidos uniformemente (distribuci√≥n homog√©nea).\n\nEval√∫a la tendencia de los cluster\nMide si un conjunto de datos contiene cluster naturales\nUtiliza un test estad√≠stico para la aleatoridad espacial\n\nLa estad√≠stica de Hopkins se calcula de la siguiente manera:\n\nMuestrea p puntos a partir del conjunto de datos\nGenerar p puntos aleatorios en el mismo espacio\nCalcula H, donde:\n\nw_i: distancia desde el punto aleatorio i hasta el vecino m√°s cercano en los datos originales\nu_i: distancia desde el punto de muestra i hasta el vecino m√°s cercano en los datos originales\n\n\n\n\n\nFormula de Estad√≠stico de Hopkins\n\n\nLos valores de H cercanos a 0,5 indican datos aleatorios, a 1 indica datos altamente agrupados y a 0 indica una distribuci√≥n uniforme.\n ## M√©todos de Detecci√≥n\nExisten diferentes m√©todos para detectar clusters:\n\nM√©todos aglomerativos (parte de cada punto)\nM√©todos divisivos (parte con todos los datos)\nM√©todos determin√≠sticos (un punto est√° asignado a un solo cluster)\nM√©todos probabil√≠sticos (prob. de pertenencia)\n\n\n\n\n\nM√©todos basados en particiones\n\n\n\n\n\n\nM√©todos jer√°rquicos\n\n\n\n\n\n\nM√©todos basados en densidad\n\n\n\n\n\n\nClusters difusos"
  },
  {
    "objectID": "resumen_cluster.html",
    "href": "resumen_cluster.html",
    "title": "4¬† Resumen de Cluster",
    "section": "",
    "text": "Clusters"
  },
  {
    "objectID": "resumen_cluster.html#objetivos",
    "href": "resumen_cluster.html#objetivos",
    "title": "4¬† Resumen de Cluster",
    "section": "4.1 Objetivos",
    "text": "4.1 Objetivos\nEncontrar algor√≠tmicamente grupos de entidades tales que: 1. La similitud intragrupo es alta 2. La similitud entre grupos es baja\nLas medidas de distancia y similitud son cruciales en este proceso"
  },
  {
    "objectID": "resumen_cluster.html#criterios-para-agrupar",
    "href": "resumen_cluster.html#criterios-para-agrupar",
    "title": "4¬† Resumen de Cluster",
    "section": "4.2 Criterios para agrupar",
    "text": "4.2 Criterios para agrupar\nExisten varias nociones de clusters basados en:\nDistancias: Cada punto est√° m√°s cerca de todos los puntos de su grupo que cualquier punto de otro grupo.\nCentroide: Cada punto est√° m√°s cerca del centro de su grupo que del centro de cualquier otro grupo.\nContig√ºidad: Cada punto est√° m√°s cerca de al menos un punto de su grupo que cualquier punto de otro grupo.\nDensidad: Los clusters son regiones de alta densidad separadas por regiones de baja densidad."
  },
  {
    "objectID": "resumen_cluster.html#preprocesamiento",
    "href": "resumen_cluster.html#preprocesamiento",
    "title": "4¬† Resumen de Cluster",
    "section": "4.3 Preprocesamiento",
    "text": "4.3 Preprocesamiento\nLos valores at√≠picos afectan el desempe√±o de la mayor√≠a de los modelos.\n\n\n\nValores At√≠picos\n\n\nLa escala de los datos tambi√©n puede jugar en contra.\n\n\n\nProblemas de Escala\n\n\nPor lo tanto, es necesario pre procesar los datos:\n\nNormalizar los datos (Distribuci√≥n normal estandar media 0 y desviaci√≥n estandar 1)\n\n\\frac{x-\\bar{x}}{\\sigma} \\frac{x-\\bar{x}}{max(x)-min(x)}\n\\frac{x-min(x)}{max(x)-min(x)} * Lidiar con valores at√≠picos\nAlternativas:\n\nSacarlos de los modelos o se le puede asignar un cluster\nAlgoritmos de clustering que admitan outliers, de los que vimos cluster difusos (probabilidad) en especifico puede ser DB-SCAN que trabaja por densidad"
  },
  {
    "objectID": "resumen_cluster.html#post-procesamiento",
    "href": "resumen_cluster.html#post-procesamiento",
    "title": "4¬† Resumen de Cluster",
    "section": "4.4 Post procesamiento",
    "text": "4.4 Post procesamiento\nAlgunos problemas se pueden resolver a trav√©s de fusi√≥n y divisi√≥n de clusters. M√©todos tradicionales generan cl√∫steres esf√©ricos.\nLa fusi√≥n de cl√∫steres mas peque√±os podr√≠a mitigar este problema. Divisi√≥n de cl√∫steres mas dispersos tambi√©n mejora el desempe√±o\n\n\n\nFusiones y Divisiones"
  },
  {
    "objectID": "resumen_cluster.html#evaluaci√≥n-de-clusters",
    "href": "resumen_cluster.html#evaluaci√≥n-de-clusters",
    "title": "4¬† Resumen de Cluster",
    "section": "4.5 Evaluaci√≥n de clusters",
    "text": "4.5 Evaluaci√≥n de clusters\n\nEstad√≠stico de Hopkins {\nInspecci√≥n visual\nCoeficiente de correlaci√≥n\nCoeficiente de cohesi√≥n\nCoeficiente de separaci√≥n\nCoeficiente de silueta\n\n\n4.5.1 Estad√≠stico de Hopkins\nAntes de agrupar un conjunto de datos, podemos probar si realmente hay cl√∫steres. Necesitamos probar la hip√≥tesis de la existencia de patrones en los datos contra un conjunto de datos distribuidos uniformemente (distribuci√≥n homog√©nea).\n\nEval√∫a la tendencia de los cluster\nMide si un conjunto de datos contiene cluster naturales\nUtiliza un test estad√≠stico para la aleatoridad espacial\n\nLa estad√≠stica de Hopkins se calcula de la siguiente manera:\n\nMuestrea p puntos a partir del conjunto de datos\nGenerar p puntos aleatorios en el mismo espacio\nCalcula H, donde:\n\nw_i: distancia desde el punto aleatorio i hasta el vecino m√°s cercano en los datos originales\nu_i: distancia desde el punto de muestra i hasta el vecino m√°s cercano en los datos originales\n\n\n\n\n\nFormula de Estad√≠stico de Hopkins\n\n\nLos valores de H cercanos a 0,5 indican datos aleatorios, a 1 indica datos altamente agrupados y a 0 indica una distribuci√≥n uniforme.\n\n\n\nVisualizaci√≥n Gr√°fica de Estad√≠stico de Hopkins\n\n\n\n\n4.5.2 Inspecci√≥n visual\n\nCrear la matriz de distancia\nOrdene la matriz en funci√≥n de las etiquetas de cl√∫ster obtenidas.\nInspeccione visualmente\n\nLas buenas agrupaciones exhiben un patr√≥n de bloque claro con ‚Äúmismo color‚Äù\n\n\n\nInspescci√≥n Visual\n\n\n\n\n4.5.3 Coeficiente de correlaci√≥n\n\nConstruir la matriz de similitud inicial entre todos los puntos s(i,j) = 1 / (1+d(i,j))\nConstruye la matriz de similitud ‚Äúideal‚Äù basada en la pertenencia al cl√∫ster\nCalcule la correlaci√≥n entre la matriz de similitud inicial y la matriz de similitud ‚Äúideal‚Äù (los ejes X e Y son la similitud inicial/ideal respectivamente).\nLa alta correlaci√≥n indica que los puntos del mismo cl√∫ster est√°n cerca el uno del otro\n\n\n\n\nCoeficiente de correlaci√≥n\n\n\n\n\n4.5.4 Coeficiente de cohesi√≥n\nMide cu√°n estrechamente relacionados est√°n los objetos dentro de cada cl√∫ster.\n\n\n\nCohesi√≥n\n\n\nSuma de errores cuadrados (SSE) es la suma de la distancia cuadrada de un punto al centroide de su cl√∫ster.\n\n\n\nFormula de Coeficiente de cohesi√≥n\n\n\n\n\n\nEsquema de Coeficiente de cohesi√≥n\n\n\n\n\n4.5.5 Coeficiente de separaci√≥n\nMide cu√°n distinto es un cl√∫ster de los otros clusters.\n\n\n\nCoeficiente de separaci√≥n\n\n\nLa suma de cuadrados intra grupo (SSB) es la suma de la distancia cuadrada de un centroide de cl√∫ster a la media general. Minimizar la cohesi√≥n equivale a maximizar la separaci√≥n.\n\n\n\nFormula de Coeficiente de separaci√≥n\n\n\n\n\n4.5.6 Coeficiente de silueta\nCombina cohesi√≥n y separaci√≥n. Normalmente var√≠a entre -1 y 1, donde\n\nValores cercanos a 1 implican una mejor agrupaci√≥n en cl√∫steres.\nValor negativo implica que el punto i est√° m√°s cerca de otro cl√∫ster\n\n\n\n\nGr√°fico de Coeficiente de silueta\n\n\nPara un punto individual i:\n\nCalcular a_i como la distancia media de i a puntos en el mismo cl√∫ster\nCalcule b_{ij} como la distancia media del punto i a todos los puntos del cl√∫ster j.\nCalcular b_i como el b_{ij} m√≠nimo excluyendo el cl√∫ster propio.\nEl coeficiente de silueta para el punto i es:\n\nS_i =\\frac{(b_i-a_i)}{max(a_i,b_i)}\n\n\n\nTabla de Coeficiente de silueta\n\n\nEl coeficiente de silueta de un cl√∫ster es el promedio de los coeficientes de silueta de los puntos pertenecientes."
  },
  {
    "objectID": "resumen_cluster.html#m√©todos-de-clusterizaci√≥n",
    "href": "resumen_cluster.html#m√©todos-de-clusterizaci√≥n",
    "title": "4¬† Resumen de Cluster",
    "section": "4.6 M√©todos de Clusterizaci√≥n",
    "text": "4.6 M√©todos de Clusterizaci√≥n\n\n4.6.1 Basados en Particiones\n\n\n\nM√©todos basados en particiones\n\n\n\n4.6.1.1 K-medias\n\nUno de los algoritmos de clusters m√°s simples.\nDado un n√∫mero K de clusters (determinado por el usuario), cada cluster est√° asociado con un centroide y cada entidad se asigna al cluster con el centroide m√°s cercano.\nVariantes como K-medioides, o K-modas usan otros estad√≠sticos como centroides\n\n\n\n\nK-medias\n\n\n\n\n\nAlgoritmo de K-medias\n\n\n\n\n\nSelecci√≥n de Centroides K-medias\n\n\n\n\n4.6.1.2 M√©todos jer√°rquicos\nLas entidades se agrupan en una jerarqu√≠a de cl√∫steres anidados.\n Los m√©todos jer√°rquicos se subdividen en aglomerativos y disociativos. Cada una de estas categor√≠as presenta una gran diversidad de variantes. 1. Los m√©todos aglomerativos, tambi√©n conocidos como ascendentes, comienzan el an√°lisis con tantos grupos como individuos haya. A partir de estas unidades iniciales se van formando grupos, de forma ascendente, hasta que al final del proceso todos los casos tratados est√°n englobados en un mismo conglomerado. 2. Los m√©todos disociativos, tambi√©n llamados descendentes, constituyen el proceso inverso al anterior. Comienzan con un conglomerado que engloba a todos los casos tratados y, a partir de este grupo inicial, a trav√©s de sucesivas divisiones, se van formando grupos cada vez m√°s peque√±os. Al final del proceso se tienen tantas agrupaciones como casos han sido tratados.\nAlgoritmo\nEl algoritmo b√°sico para clustering aglomerativo es sencillo:\n\nDeje que cada punto de datos sea un cl√∫ster\nCalcular la matriz de proximidad (matriz de distancia entre cada cl√∫ster)\nRepetir hasta que s√≥lo quede un solo cl√∫ster\n\nFusionar los dos cl√∫steres m√°s cercanos\nActualizar la matriz de proximidad\n\n\nEl paso clave es el c√°lculo de la proximidad de dos cl√∫steres\nDiferentes enfoques para definir la distancia entre cl√∫steres distinguen los diferentes algoritmos\nDendrogramas\nUn dendrograma es una estructura de tipo √°rbol que muestra el proceso generativo de agrupaci√≥n en cl√∫steres. El eje X muestra los puntos de datos originales, mientras que el eje Y podr√≠a mostrar la distancia entre cl√∫steres.\n\n\n\nTipos de Dendogramas\n\n\nM√©todos de Aglomeraci√≥n\nA continuaci√≥n vamos a presentar algunas de las estrategias que pueden ser empleadas a la hora de unir los clusters en las diversas etapas o niveles de un procedimiento jer√°rquico. Ninguno de estos procedimientos proporciona una soluci√≥n √≥ptima para todos los problemas que se pueden plantear, ya que es posible llegar a distintos resultados seg√∫n el m√©todo elegido.\n\nSingle linkage:\n\nEsta estrategia recibe en la literatura anglosajona el nombre de amalgamamiento simple (single linkage). En este m√©todo se considera que la distancia o similitud entre dos clusters viene dada, respectivamente, por la m√≠nima distancia (o m√°xima similitud) entre sus componentes.\n\nComplete linkage:\n\nEn este m√©todo, tambi√©n conocido como el procedimiento de amalgamamiento completo (complete linkage), se considera que la distancia o similitud entre dos clusters hay que medirla atendiendo a sus elementos m√°s dispares, o sea, la distancia o similitud entre clusters viene dada, respectivamente, por la m√°xima distancia (o m√≠nima similitud) entre sus componentes.\n\nAverage linkage:\n\nEn esta estrategia la distancia, o similitud, del cluster C_i con el C_j se obtiene como la media aritm√©tica entre la distancia, o similitud, de las componentes de dichos clusters.\n\n\n\n\n\nM√©todos de Aglomeraci√≥n\n\n\nDesventajas\n\nEl algoritmo es demasiado caro O(n3): Hay n pasos, para unir cl√∫steres, y en cada paso calculamos la matriz de proximidad O(n2).\nUna vez que se toma la decisi√≥n de combinar dos grupos, no se puede revertir\nNinguna funci√≥n objetivo se minimiza directamente\n\n\n\n\n4.6.2 Clusters difusos\nCada punto tiene un probabilidad de pertenecer a cada cl√∫ster.\n\n\n\nEjemplo de Clusters difusos\n\n\n\n4.6.2.1 Coeficiente de partici√≥n difusa (FPC)\nEl coeficiente de partici√≥n difuso eval√∫a la variabilidad de las asignaciones.\nF(U)= \\frac{tr(U*U^T)}{n}\nDonde U es la matriz de pertenencia, * es el operador de multiplicaci√≥n entre matrices, y tr( ) es la traza de la matriz, es decir, la suma de los valores diagonales.\nEl coeficiente de partici√≥n difusa var√≠a entre 0 y 1 donde un valor cercano a 1 implica menor variabilidad en la matriz de pertenencia, que se asocia a una mejor clusterizaci√≥n de los datos.\n\n\n\nValores de Coeficiente de partici√≥n difusa y su representaci√≥n gr√°fica\n\n\n\n\n4.6.2.2 Gaussian Mixed Models\nUn modelo de mezcla gaussiana (GMM) asume que los datos se generaron a partir de una mezcla de K gaussianos multidimensionales, donde cada componente tiene par√°metros: N_k(\\mu_k,\\Sigma_k)\n\nK es definido por el usuario.*\n\n\n\n\nRepresenaci√≥n Gr√°fica Gaussian Mixed Models\n\n\nGaussiono Multivariado\n\n\n\nGaussionao Multivariado\n\n\n Algoritmo\n\n\n\nAlgoritmo GMM"
  },
  {
    "objectID": "resumen_cluster.html#referencias",
    "href": "resumen_cluster.html#referencias",
    "title": "4¬† Resumen de Cluster",
    "section": "4.7 Referencias",
    "text": "4.7 Referencias\nM√©todos Jer√°rquicos de An√°lisis Cluster."
  },
  {
    "objectID": "res_a_supervisado.html",
    "href": "res_a_supervisado.html",
    "title": "5¬† Resumen A. Supervisado",
    "section": "",
    "text": "k-NN es un tipo de aprendizaje basado en instancias o aprendizaje perezoso (lazy).\nk-NN almacena los datos de entrenamiento p-dimensionales y retrasa el proceso de aprendizaje hasta que se debe clasificar una nueva instancia.\nPara predecir un nuevo punto, los vecinos k m√°s cercanos se calculan utilizando la distancia\nLa clasificaci√≥n final se realiza en funci√≥n de las etiquetas de clase de estos vecinos.\n\n\n\n\nEjemplo de clasificaci√≥n con k-NN con sus fronteras de decisi√≥n\n\n\n\n\n\n\nDeje que x_j sea un nuevo punto p-dimensional sin etiquetar.\nCalcular d(x_i, x_j) para i={1,...,n}\nEncuentra los vecinos k m√°s cercanos de x_j (d(x_i, x_j) se minimiza).\nEtiqueta x_j basada en los k vecinos m√°s cercanos\n\n\n\n\n\n¬øCu√°l es la mejor relaci√≥n calidad-precio para k?\n\nPor lo general, se utiliza un valor peque√±o, por ejemplo, k<10.\n\n¬øQu√© medida de distancia d( ) utilizar?\n\nA menudo se utiliza la distancia euclidiana (L2).\n\n¬øC√≥mo puedo etiquetar xj en funci√≥n de los vecinos k m√°s cercanos?\n\nA menudo se utiliza el voto mayoritario.\n\n\n\n\n\nPar√°metros del modelo:\n\nk (n√∫mero de vecinos)\ncualquier par√°metro de medida de distancia (por ejemplo, pesos en las entidades)\n\nFortalezas:\n\nModelo sencillo, f√°cil de implementar\nAprendizaje muy eficiente: O(1)\n\nDebilidades:\n\nInferencia ineficiente: tiempo y espacio O(n)\nMaldici√≥n de la dimensionalidad: A medida que aumenta el n√∫mero de entidades, necesita un aumento exponencial en el tama√±o de los datos para asegurarse de que tiene ejemplos cercanos para cualquier dato dado."
  },
  {
    "objectID": "res_a_supervisado.html#clasificador-bayesiano",
    "href": "res_a_supervisado.html#clasificador-bayesiano",
    "title": "5¬† Resumen A. Supervisado",
    "section": "5.2 Clasificador bayesiano",
    "text": "5.2 Clasificador bayesiano\n\n5.2.1 Naive Bayes\nNaive Bayes aprende una distribuci√≥n condicional de la probabilidad. Dado un punto de datos x, la salida del modelo es la probabilidad que x pertenezca a una clase espec√≠fica.\nNaive Bayes utiliza 3 conceptos clave:\n\nProbabilidad condicional\nTeorema bayesiano\nIndependencia condicional\n\n\n\n\n¬øQu√© tan probable es comprar una computadora a un estudiante de 23 a√±os con un cr√©dito justo y ingresos medios? x=\\{<=30,medium,yes,fair\\}\n\n\n\nTruco:\n\nAsumiremos Ingenuamente independencia condicional de X_1, X_2, ...,X_k dado C (aunque esto no necesariamente sea cierto)\n\n\nP(C|X_1, X_2, ...,X_k)\\propto \\prod_{i=1}^{k}P(X_i|C)P(C)\nAhora f√°cilmente podemos calcular cada P(X_i|C) a partir de los datos.\n\n\n5.2.2 Algoritmo de Naive Bayes\n\nFase de aprendizaje: Dado un conjunto de entrenamiento S,\n\n\nSalida: Tablas de Probabilidad condicional; para elemento x_j, X_j \\times L\n\nFase de prueba: dada una instancia desconocida X' =(a'_1,...,a'_n)\n\nBuscar tablas para asignar la etiqueta c* a X' si:\n\n\n\n5.2.3 Caracter√≠sticas de Naive Bayes\nFortalezas:\n\nF√°cil de implementar y se puede aprender de forma incremental\nA menudo funciona bien incluso cuando se viola la suposici√≥n de independencia\nSe puede aprender gradualmente\nLos valores que faltan se ignoran en el proceso de aprendizaje\nModelo robusto con respecto a valores at√≠picos y datos irrelevantes\n\nDebilidades:\n\nLa suposici√≥n condicional de clase produce estimaciones de probabilidad sesgadas\nLas dependencias entre variables no se pueden modelar\n\n\n\n5.2.4 Conclusiones de Naive Bayes\n\nNaive Bayes se basa en el supuesto de la independencia\n\nEl entrenamiento es muy f√°cil y r√°pido; solo requiere considerar cada atributo en cada clase por separado\nLa prueba es sencilla; simplemente buscando tablas o calculando probabilidades condicionales con distribuciones normales\n\nUn modelo generativo popular\n\nPerformance es competitivo para la mayor√≠a de los clasificadores de √∫ltima generaci√≥n, incluso en presencia de una violaci√≥n de la suposici√≥n de independencia\nMuchas aplicaciones exitosas, por ejemplo, filtrado de correo no deseado"
  },
  {
    "objectID": "res_a_supervisado.html#√°rboles-de-decisi√≥n",
    "href": "res_a_supervisado.html#√°rboles-de-decisi√≥n",
    "title": "5¬† Resumen A. Supervisado",
    "section": "5.3 √Årboles de Decisi√≥n",
    "text": "5.3 √Årboles de Decisi√≥n\n\n5.3.1 Estructura de un √Årbol de Decisi√≥n\nUn √°rbol de decisi√≥n es una estructura similar a un diagrama de flujo en la que:\n\nCada nodo interno representa una ‚Äúprueba‚Äù en un atributo\nCada rama representa el resultado de la prueba\nCada nodo hoja representa una etiqueta de clase\n\nDado un punto de datos x, la salida del modelo es la probabilidad de que x pertenezca a una clase espec√≠fica.\n\n\n\nEjemplo de un √°rbol de decisi√≥n\n\n\n\n\n5.3.2 Definici√≥n\nUn √°rbol de decisi√≥n segmenta los datos de entrada utilizando secciones rectangulares del espacio\n\n\n\nSegmentaci√≥n de datos en secciones rectangulares\n\n\n\n\n5.3.3 Algoritmo de Hunt\nSea D_t el conjunto de registros de entrenamiento que llegan a un nodo T\n\nsi D_t contiene registros que pertenecen a la misma clase y_t, entonces t es un nodo hoja etiquetado como y_t\nSi D_t es un conjunto vac√≠o, entonces t es un nodo hoja etiquetado por la clase predeterminada, y_d\nSi D_t contiene registros que pertenecen a m√°s de una clase, utilice una prueba de atributo para dividir los datos en subconjuntos m√°s peque√±os. Aplicar el procedimiento de forma recursiva a cada subconjunto.\n\n\n\n\nDefinici√≥n de Algoritmo de Hunt\n\n\n\n\n5.3.4 Consideraciones en √Årboles de Decisi√≥n\nC√≥mo especificar los puntos de corte de atributos\nN√∫mero de formas de dividir\n\nDivisi√≥n de 2 v√≠as\nDivisi√≥n multi v√≠as\n\nTipos de atributos\n\nNominal\nOrdinal\nContinuo\n\nParticionamiento de atributo continuo\n\nPartici√≥n binaria: Regla √∫nica que divide el atributo en dos subconjuntos (X_j > v). Podr√≠a ser computacionalmente costoso, porque debe considerar todas las divisiones posibles y encontrar el mejor corte.\n\n\n\n\nPartici√≥n binaria de una variable continua\n\n\n\nDecisi√≥n de discretizaci√≥n: Formar un atributo categ√≥rico ordinal\n\nEst√°tico: discretizar una vez al principio (edad<30, 30<edad<40, 40<edad)\nDin√°mico: los rangos pueden variar dependiendo de la rama del √°rbol\n\n\n\n\n\nDiscretizaci√≥n de una variable continua\n\n\nM√©todos de crecimiento completo\n\nTodos los ejemplos de un nodo pertenecen a la misma clase\nNo quedan atributos para divisiones posteriores\nNo quedan muestras\n\n¬øQu√© impacto tiene esto en la calidad de los √°rboles aprendidos?\n\nLos √°rboles sobreajustan los datos y la precisi√≥n de las pruebas disminuye.\nEl modelo aprende los datos de entrenamiento, pero no se generaliza a los nuevos datos.\nLa poda se utiliza para evitar el sobreajuste.\n\nPoda\nPre poda\n\nAplicar una prueba estad√≠stica para decidir si se debe expandir un nodo\nUtilice una medida expl√≠cita de complejidad para penalizar los √°rboles grandes (por ejemplo, longitud m√≠nima de la descripci√≥n)\n\nPost poda\n\nUtilice un conjunto de ejemplos para evaluar la utilidad de podar nodos del √°rbol (despu√©s de que el √°rbol est√© completamente crecido)"
  },
  {
    "objectID": "res_a_supervisado.html#m√°quinas-de-soporte-vectorial",
    "href": "res_a_supervisado.html#m√°quinas-de-soporte-vectorial",
    "title": "5¬† Resumen A. Supervisado",
    "section": "5.4 M√°quinas de soporte vectorial",
    "text": "5.4 M√°quinas de soporte vectorial\n\n5.4.1 Definici√≥n Support Vector Machines (SVM)\nSVM es un modelo discriminativo que busca la mejor manera de dividir un espacio, bas√°ndose en algunos puntos de este (support vectors), para separarlo en 2 zonas con clases distintas.\nLuego se construye un clasificador sobre esa divisi√≥n.\n\n\n\nDivisi√≥n de el espacio usando vectores de soporte\n\n\n\n\n5.4.2 Descripci√≥n de SVM\nPara separar el espacio nosotros tenemos que buscar un hiperplano (de una dimensi√≥n menor a las dimensiones del espacio). En un espacio 2D, como en el pr√≥ximo ejemplo, el hiperplano ser√° una l√≠nea.\n\n\n\nLinea (hiperplano) que divide el espacio como frontera de decisi√≥n\n\n\nCuando llegue un nuevo punto, podemos evaluar a que lado del hiperplano esta. Dependiendo de a que lado este, lo podemos clasificar (como rojo o azul en este caso).\n\nEl problema es que muchos hiperplanos logran dividir el espacio en 2. ¬øCu√°l ser√° el mejor? ¬øC√≥mo lo logramos encontrar? ¬øQu√© factores afectan su posici√≥n?\n\n\n\n5.4.3 Frontera de Decisi√≥n SVM\nA cada posible hiperplano que cumpla las condiciones, le dibujaremos un margen equidistante a ambos lados hasta tocar el primer punto del espacio.\nDiremos que el mejor hiperplano es aquel que tenga el margen mas grande posible. Esto lo podemos denominar el m√©todo del ‚Äúancho de avenida‚Äù.\n Entre el hiperplano representado por la ecuaci√≥n de la recta de color naranjo tiene un margen mas grande que el rosado; por lo tanto esta dividiendo el espacio de mejor forma. Los ‚Äúvectores de soporte‚Äù son los puntos que tienen contado con los m√°rgenes del hiperplano.\n\n\n5.4.4 Optimizaci√≥n SVM\nEl problema es de minimizaci√≥n de tipo convexo cuadr√°tico, con muchas restricciones lineales de inigualdades. Con p+1 par√°metros (p es la dimensionalidad de la data) y n restricciones (una restricci√≥n por cada entidad). \nEn este caso podemos usar el m√©todo de optimizaci√≥n de Lagrange para maximizar una nueva funci√≥n y no preocuparnos de las restricciones. Las restricciones ser√°n reemplazadas por multiplicadores de Lagrange y el proceso de aprendizaje ser√° dictado por productos puntos.\n\n\n\nOptimizaci√≥n de Lagrange para mazimizar una nueva funci√≥n\n\n\nLagrangiano\nDefinici√≥n de L:\n\n\n\nDefinici√≥n de L\n\n\nConfiguraci√≥n de gradientes de L respecto a incoÃÅgnitas (W y b):\n\n\n\nConfiguraci√≥n de gradientes\n\n\nRestricci√≥n de no-negatividad de los multiplicadores de Lagrange:\n\\alpha \\geq 0 \\space \\forall i \\in \\{1,2, .., n\\} \n\n\n5.4.5 Interpretaci√≥n geom√©trica SVM\n\n\n\nInterpretaci√≥n geom√©trica de Support Vector Machine\n\n\n\n\n5.4.6 Caso no separable SVM\nSVM, como lo hemos visto hasta ahora tiene un primer problema. No siempre se puede dividir el espacio con un hiperplano que cumpla las condiciones:\ny_i(w \\cdot X_i+b)-1 \\geq 0\n\nPara solventar esto, podemos agregar a SVM una ‚Äúholgura‚Äù, que permita soportar una determinada cantidad de puntos en el incorrecto lado de la calle; con el fin de lograr generar el hiperplano.\n\n\n\n\nCaso no separable\n\n\nAl someter la nueva funcioÃÅn de optimizacioÃÅn, con sus restricciones, al Lagrange, obtenemos un resultado praÃÅcticamente igual:\n\n\n\nOptimizaci√≥n para el caso no separable\n\n\nSensibilidad de C\n\n\n\nSensibilidad de C\n\n\n\n\n5.4.7 Limitaciones SVM cl√°sico\nImagina que aplicamos SVM sobre el pr√≥ximo problema. Puedes seleccionar cualquier C que quieras. Dibuja la mejor l√≠nea de separaci√≥n.\n\n\n\nAunque hay claramente 2 clases distintas, no podemos dividir el espacio con el hiperplano de ninguna manera que nos de un buen resultado.\nEs un problema donde necesitamos dividir el espacio de una manera no lineal.\n\nTruco del Kernel\nSVM normal funciona bien para DataSet que se pueden dividir linealmente (aunque tengan un poco de ruido, para eso usamos C):\n\n¬øPero que hacemos con data set donde no podemos separar linealmente?\n\n‚ÄúMapeamos‚Äù o transformamos el DataSet a un dimensi√≥n mayor, y en ese lugar buscamos si podemos encontrar un hiperplano y aplicar SVM.\n\nNuestro nuevo problema de optimizacioÃÅn, considerando una funcioÃÅn de Kernel se ve asiÃÅ:\n\nExisten cientos de Kernels, distintos (es un aÃÅrea de estudio acadeÃÅmico):\n\nLinear => K(x_i,x_j) = x_i^Tx_j\nPolynomial kernel with degree d => K(x_i,x_j) = (x_i^T x_j+1)^d\nRadial basis function kernel with width \\sigma => K(x_i,x_j) = exp(-||x_i-x_j||^2/(2\\sigma^2))\nSigmoid with parameter Œ∫ and Œ∏ => K(x_i,x_j) = tanh(Œ∫*x_i^T x_j + Œ∏)\n\n\n\n5.4.8 Debilidades y fortalezas SVM\nDebilidades:\n\nEl proceso de entrenamiento y prueba es lento, debido a tener que solucionar un problema de Lagrange.\nEn casi todas sus variedades solo puede clasificar de manera binaria (+1,-1)\nMuy sensitivo al ruido.\nLo peor: lograr escoger la funci√≥n de Kernel correcta.\n\nFortalezas:\n\nEl entrenamiento es f√°cil, la soluci√≥n es √∫nica y global para todo el espacio.\nSVM no sufre de la maldici√≥n de la dimensionalidad !\nNo se genera sobretratamiento de manera muy f√°cil.\nF√°cil de entender de manera geom√©trica.\n\n\n\n5.4.9 Modelos Ensamblados\n\n5.4.9.1 Dilema sesgo-varianza\nEl error de predicci√≥n de cualquier modelo de machine learning puede ser separado en tres t√©rminos:\n\nVarianza/Variance: ¬øqu√© tan factible es que cambia nuestra prediccioÃÅn si usamos otro set de datos?. Se relaciona a la complejidad del modelo.\nRuido/Noise: error base imposible de reducir (variables desconocidas, problemas de adquisicioÃÅn de datos, etc).\nSesgo/Bias: ¬øqueÃÅ tan lejana es nuestra prediccioÃÅn promedio con respecto al verdadero valor? Se relaciona a la suposiciones/simplicidad del modelo.\n\n\n\n\n\nSesgo/varianza\n\n\n\n\n\nRelaciones en un modelo\n\n\n\n\n5.4.9.2 Introducci√≥n\nEs muy dif√≠cil construir un solo modelo que obtenga un buen rendimiento (bajo sesgo y varianza).\nEnfoque: construyamos m√∫ltiples modelos ‚Äúcomplejos‚Äù o ‚Äúsencillos‚Äù con el mismo o distinto set de datos y combinemos sus predicciones.\nObjetivo: obtener ‚Äúun modelo final‚Äù con bajo error de sesgo y varianza.\n\nModelos complejos => bajo error de sesgo y alto error de varianza. Combinaci√≥n de predicciones => bajar la varianza de la predicci√≥n.\nModelos sencillos => alto error de sesgo y bajo error de varianza. Combinaci√≥n de predicciones => mantendr√≠a el alto sesgo, excepto que los modelos se enfoquen en los puntos mal clasificados.\n\n\n\n\n5.4.9.3 Problema multi-clase\nProblemas con m√∫ltiple clases son usualmente complejos. Un problema multi-clase se puede simplificar a m√∫ltiples problemas binarios\n\nUn problema con m clases puede ser resuelto con m modelos, calculando P(C_i|M_i), donde Mi es un modelo binario para la clase i.\nUn problema con m clases puede ser resuelto con \\frac{m(m-1)}{2} modelos, donde cada modelo se enfoca solo entre dos clases.\n\nLa predicci√≥n final estar√° dada por:\n\nMayor probabilidad.\nMayor√≠a.\nVoto ponderado.\n\n\n\n\n5.4.9.4 Bootstrap aggregating (Bagging)\n\n\n\n5.4.9.5 Bagging\nDado un set de entrenamiento D={(x1,y1),..., (xN,yN)}, y M modelos\nFor m in range(M):\nObtener una muestra bootstrap (samplear con repetici√≥n N puntos) del set de datos D, generando la muestra Dm. Aprender el modelo Mm de Dm\nPara clasificar el punto xt se aplica cada uno de los M modelos (M1,M2,‚Ä¶,Mm) a xt y se considera la clase con m√°s votos o un promedio (ponderado).\nLos modelos tienen errores no correlacionados debido a la diversidad de puntos en cada muestra bootstrap\nEjemplo\n\n\n\n5.4.9.6 Random forest\nUno de los modelos de ensamblados m√°s conocidos es el random forest, el cual es un ensamblado de √°rboles de decisi√≥n.\n\n\nEl proceso de combinaci√≥n puede ser promedio simple o mayor√≠a.\nCada √°rbol de decisi√≥n ser√° ‚Äúdistinto‚Äù (limitando el n√∫mero de variables) en comparaci√≥n a un puro √°rbol de decisi√≥n.\nEl uso de m√∫ltiples √°rboles ‚Äúdistintos‚Äù y complejos nos permite reducir la componente de la varianza del error.\n\n\n\n5.4.9.7 Boosting\n\nAsignar a cada punto de entrenamiento D={(x_1,y_1),..., (x_N,y_N)}, un peso igual a w_i = 1/N, obteniendo el vector de probabilidades w.\nFor m in range(M):\n\nGenerar Dm usando los pesos w Aprender el modelo Mm de Dm\nCalcular el error de Mm y cambiar los pesos wi de los puntos incorrectamente clasificados\nNormalizar los pesos wi para que sumen 1\nEstimar la importancia del modelo ùõÇm basado en los errores\nPara clasificar el punto xt se aplica cada uno de los M modelos (M1,M2,‚Ä¶,Mm) a xt y se considera un promedio ponderado usando ùõÇm.\n\n\n\n5.4.9.8 Adaboost\nUno de los algoritmos m√°s famosos de boosting es adaboost, el cual es un ensamblado de modelos sencillos, donde:\n\n\nLa predicci√≥n final es realizada con un promedio ponderado, basado en el error del modelo.\nCada modelo ser√° sencillo (obteniendo un alto sesgo).\nEl uso de modelos sencillos reduce la varianza del error.\nEl foco en puntos mal clasificados reduce el sesgo del error.\n\n\nEjemplo:\nPrimera Iteraci√≥n\n\nSegunda Iteraci√≥n\n\nFinal\n\n\n\n5.4.9.9 Gradient boosting\nGadient boosting, a diferencia de los modelos previos, se enfoca en predecir en forma correcta los errores de los modelos anteriores.\n\n\nLa predicci√≥n final corresponde a la suma de todos los modelos.\nCada modelo ser√° sencillo (obteniendo un alto sesgo).\nEl uso de modelos sencillos reduce la varianza del error.\nEl foco en los errores de clasificaciones previas reduce el sesgo del error. Sin embargo, esto puede causar sobreentrenamiento.\n\nAlgoritmo de Gradiente Boosting\n\n\n\n5.4.9.10 Regressor tree\nUn regressor tree es un √°rbol de decisi√≥n enfocado a regresi√≥n, donde cada separaci√≥n busca generar grupos similares de datos.\nEn vez de ‚Äúgini‚Äù o ‚Äúentropy‚Äù el criterio a minimizar es el ‚Äúsquared_error‚Äù que corresponde al error cuadr√°tico medio (otro criterios existen).\nEn la pr√°ctica esto es equivalente a buscar una separaci√≥n que minimice la varianza de los datos dentro de cada hoja. La ‚Äúclasificaci√≥n‚Äù final corresponde al promedio de los datos de cada hoja."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "6¬† Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  }
]