[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning 2022",
    "section": "",
    "text": "Registrar avances generales del curso de Machine Learning en Data Science"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introducción",
    "section": "",
    "text": "Identificar elementos clave de ciencia de datos\nComprender cómo interactúan los elementos algorítmicos para afectar el rendimiento\nComprender cómo elegir algoritmos para diferentes tareas de análisis\nAnalizar los datos de una manera exploratoria y específica\nImplementar y aplicar algoritmos básicos para el aprendizaje supervisado y sin supervisión\nEvaluar con precisión el rendimiento de los algoritmos."
  },
  {
    "objectID": "intro.html#qué-es-data-science",
    "href": "intro.html#qué-es-data-science",
    "title": "Introducción",
    "section": "¿Qué es Data Science?",
    "text": "¿Qué es Data Science?\n\n\n\nData Science\n\n\n\nAnalytics:\n\nUso de datos y métodos cuantitativos para mejorar la toma de decisiones"
  },
  {
    "objectID": "intro.html#niveles-de-información",
    "href": "intro.html#niveles-de-información",
    "title": "Introducción",
    "section": "Niveles de Información",
    "text": "Niveles de Información\n\n\n\nNiveles Información"
  },
  {
    "objectID": "intro.html#flujos-de-información",
    "href": "intro.html#flujos-de-información",
    "title": "Introducción",
    "section": "Flujos de Información",
    "text": "Flujos de Información\n\n\n\nFlujos de Información"
  },
  {
    "objectID": "intro.html#niveles-de-ia",
    "href": "intro.html#niveles-de-ia",
    "title": "Introducción",
    "section": "Niveles de IA",
    "text": "Niveles de IA\n\nSistemas automáticos\nRedes Neuronales Artificiales\nRobótica Cognitiva"
  },
  {
    "objectID": "intro.html#tipos-de-aprendizaje-automático",
    "href": "intro.html#tipos-de-aprendizaje-automático",
    "title": "Introducción",
    "section": "Tipos de aprendizaje automático",
    "text": "Tipos de aprendizaje automático\n\n\n\nTipos de ML"
  },
  {
    "objectID": "intro.html#esquema-conceptual",
    "href": "intro.html#esquema-conceptual",
    "title": "Introducción",
    "section": "Esquema conceptual",
    "text": "Esquema conceptual\n\n\n\nConceptos en Machine Learning"
  },
  {
    "objectID": "C2-Features_engineering.html",
    "href": "C2-Features_engineering.html",
    "title": "2  Feature Engineering",
    "section": "",
    "text": "Una cantidad correcta de atributos ayudan a crear mejores modelos.\nLos datos de altas dimensiones se vuelven cada vez más dispersos en su espacio.\nLas definiciones de densidad y distancia entre puntos se vuelven menos significativas a mayor numero de atributos.\n\n\n\n\nLa Maldición de la dimensionalidad"
  },
  {
    "objectID": "C2-Features_engineering.html#análisis-exploratorio-de-datos-eda",
    "href": "C2-Features_engineering.html#análisis-exploratorio-de-datos-eda",
    "title": "2  Feature Engineering",
    "section": "2.2 Análisis exploratorio de datos (EDA)",
    "text": "2.2 Análisis exploratorio de datos (EDA)\nAnálisis del conjunto de datos para resumir sus principales características, mediante métodos estadísticos y visuales.\n\n2.2.1 Objetivos EDA:\n\nDescubrir la estructura subyacente de los datos\nIdentificar variables relevantes Detectar valores atípicos y anomalías Validar supuestos\nGenerar hipótesis a partir de los datos\n\n\n\n\nEjemplo de Análisis exploratorio de datos (EDA)"
  },
  {
    "objectID": "C2-Features_engineering.html#definición-de-ingeniería-de-atributos",
    "href": "C2-Features_engineering.html#definición-de-ingeniería-de-atributos",
    "title": "2  Feature Engineering",
    "section": "2.3 Definición de Ingeniería de Atributos",
    "text": "2.3 Definición de Ingeniería de Atributos\n\nSelección de atributos:\n\nSelección de un subconjunto de atributos según algún criterio específico.\n\n\n\n\n\nSelección de atributos\n\n\n\nExtracción de atributos:\n\nCreación de nuevos atributos a partir de atributos originales\n\n\n\n\n\nExtracción de atributos\n\n\nPueden hacerse con conocimiento del dominio o algorítmicamente"
  },
  {
    "objectID": "C2-Features_engineering.html#objetivos-de-feature-engineering",
    "href": "C2-Features_engineering.html#objetivos-de-feature-engineering",
    "title": "2  Feature Engineering",
    "section": "2.4 Objetivos de Feature Engineering",
    "text": "2.4 Objetivos de Feature Engineering\n\nMejorar el desempeño de los modelos:\n\nPoder predictivo\nComplejidad\nTiempo de ejecución\n\nVisualizar los datos\nEliminar el ruido"
  },
  {
    "objectID": "C2-Features_engineering.html#sec-distancias",
    "href": "C2-Features_engineering.html#sec-distancias",
    "title": "2  Feature Engineering",
    "section": "2.5 Distancias",
    "text": "2.5 Distancias\nUna métrica que mide la distancia entre un par de entidades dados los dos puntos x e y, una función métrica o de distancia debe cumplir las siguientes condiciones:\n\nTabla de Condiciones de Distancias\n\n\nNombre Condición\nCondición\n\n\n\n\nNo negatividad\nd(x,y) >0\n\n\nIdentidad\nd(x,y)=0 <=> x=y\n\n\nSimetría\nd(x,y)=d(y,x)\n\n\nDesigualdad triangular\nd(x,z) <= d(x,y)+d(y,z)\n\n\n\n\n2.5.1 Tipos de Distancias\n\n2.5.1.1 Distancia Euclediana\nSe desprende del teorema de Pitágoras\n\n\n\nDistancia Euclediana\n\n\nd_E(P_1, P_2)=\\sqrt{(x_2-x_1)^2+(y_2-y_1)^2}\n\n\n2.5.1.2 Distancia de Minkowski\nGeneralización de la distancia euclidiana. Sean que p y q vectores m-dimensionales\n\n\n\nFormulación de Distancia de Minkowski\n\n\n\n\n2.5.1.3 Distancia de Mahalanobis\n\n\n\nFórmulación de Distancia de Minkowski\n\n\n\n\n2.5.1.4 Distancia del Coseno\n #### Distancia Suave del Coseno\n\n\n\nDistancia Suave del Coseno\n\n\n\n\n\n2.5.2 Matriz de Distancias\n\n\n\nMatriz de Distancias"
  },
  {
    "objectID": "C2-Features_engineering.html#similitud",
    "href": "C2-Features_engineering.html#similitud",
    "title": "2  Feature Engineering",
    "section": "2.6 Similitud",
    "text": "2.6 Similitud\nSimilitud:\n\nMide lo parecidos que son dos entidades.\nEs más alto cuando entidades son más parecidas.\n\nDisimilitud\n\nMide cuan diferentes son dos entidades\nMás bajo cuando los objetos son más parecidos\n\n\n\n\nTabla de Similitud y Disimilitud"
  },
  {
    "objectID": "C2-Features_engineering.html#referencias",
    "href": "C2-Features_engineering.html#referencias",
    "title": "2  Feature Engineering",
    "section": "2.7 Referencias:",
    "text": "2.7 Referencias:\nÍndice de Cálculo de Distancias"
  },
  {
    "objectID": "C3-clusters.html",
    "href": "C3-clusters.html",
    "title": "3  Análisis de Cluster",
    "section": "",
    "text": "Clusters"
  },
  {
    "objectID": "C3-clusters.html#objetivos",
    "href": "C3-clusters.html#objetivos",
    "title": "3  Análisis de Cluster",
    "section": "3.1 Objetivos",
    "text": "3.1 Objetivos\nEncontrar algorítmicamente grupos de entidades tales que:\n\nLa similitud intragrupo es alta\nLa similitud entre grupos es baja\n\nLas medidas de distancia y similitud son cruciales en este proceso"
  },
  {
    "objectID": "C3-clusters.html#criterios-para-agrupar",
    "href": "C3-clusters.html#criterios-para-agrupar",
    "title": "3  Análisis de Cluster",
    "section": "3.2 Criterios para agrupar",
    "text": "3.2 Criterios para agrupar\nExisten varias nociones de clusters basados en:\nDistancias: Cada punto está más cerca de todos los puntos de su grupo que cualquier punto de otro grupo.\nCentroide: Cada punto está más cerca del centro de su grupo que del centro de cualquier otro grupo.\nContigüidad: Cada punto está más cerca de al menos un punto de su grupo que cualquier punto de otro grupo.\nDensidad: Los clusters son regiones de alta densidad separadas por regiones de baja densidad."
  },
  {
    "objectID": "C3-clusters.html#estadístico-de-hopkins",
    "href": "C3-clusters.html#estadístico-de-hopkins",
    "title": "3  Análisis de Cluster",
    "section": "3.3 Estadístico de Hopkins",
    "text": "3.3 Estadístico de Hopkins\nAntes de agrupar un conjunto de datos, podemos probar si realmente hay clústeres. Necesitamos probar la hipótesis de la existencia de patrones en los datos contra un conjunto de datos distribuidos uniformemente (distribución homogénea).\n\nEvalúa la tendencia de los cluster\nMide si un conjunto de datos contiene cluster naturales\nUtiliza un test estadístico para la aleatoridad espacial\n\nLa estadística de Hopkins se calcula de la siguiente manera:\n\nMuestrea p puntos a partir del conjunto de datos\nGenerar p puntos aleatorios en el mismo espacio\nCalcula H, donde:\n\nw_i: distancia desde el punto aleatorio i hasta el vecino más cercano en los datos originales\nu_i: distancia desde el punto de muestra i hasta el vecino más cercano en los datos originales\n\n\n\n\n\nFormula de Estadístico de Hopkins\n\n\nLos valores de H cercanos a 0,5 indican datos aleatorios, a 1 indica datos altamente agrupados y a 0 indica una distribución uniforme.\n ## Métodos de Detección\nExisten diferentes métodos para detectar clusters:\n\nMétodos aglomerativos (parte de cada punto)\nMétodos divisivos (parte con todos los datos)\nMétodos determinísticos (un punto está asignado a un solo cluster)\nMétodos probabilísticos (prob. de pertenencia)\n\n\n\n\n\nMétodos basados en particiones\n\n\n\n\n\n\nMétodos jerárquicos\n\n\n\n\n\n\nMétodos basados en densidad\n\n\n\n\n\n\nClusters difusos"
  },
  {
    "objectID": "resumen_cluster.html",
    "href": "resumen_cluster.html",
    "title": "4  Resumen de Cluster",
    "section": "",
    "text": "Clusters"
  },
  {
    "objectID": "resumen_cluster.html#objetivos",
    "href": "resumen_cluster.html#objetivos",
    "title": "4  Resumen de Cluster",
    "section": "4.1 Objetivos",
    "text": "4.1 Objetivos\nEncontrar algorítmicamente grupos de entidades tales que: 1. La similitud intragrupo es alta 2. La similitud entre grupos es baja\nLas medidas de distancia y similitud son cruciales en este proceso"
  },
  {
    "objectID": "resumen_cluster.html#criterios-para-agrupar",
    "href": "resumen_cluster.html#criterios-para-agrupar",
    "title": "4  Resumen de Cluster",
    "section": "4.2 Criterios para agrupar",
    "text": "4.2 Criterios para agrupar\nExisten varias nociones de clusters basados en:\nDistancias: Cada punto está más cerca de todos los puntos de su grupo que cualquier punto de otro grupo.\nCentroide: Cada punto está más cerca del centro de su grupo que del centro de cualquier otro grupo.\nContigüidad: Cada punto está más cerca de al menos un punto de su grupo que cualquier punto de otro grupo.\nDensidad: Los clusters son regiones de alta densidad separadas por regiones de baja densidad."
  },
  {
    "objectID": "resumen_cluster.html#preprocesamiento",
    "href": "resumen_cluster.html#preprocesamiento",
    "title": "4  Resumen de Cluster",
    "section": "4.3 Preprocesamiento",
    "text": "4.3 Preprocesamiento\nLos valores atípicos afectan el desempeño de la mayoría de los modelos.\n\n\n\nValores Atípicos\n\n\nLa escala de los datos también puede jugar en contra.\n\n\n\nProblemas de Escala\n\n\nPor lo tanto, es necesario pre procesar los datos:\n\nNormalizar los datos (Distribución normal estandar media 0 y desviación estandar 1)\n\n\\frac{x-\\bar{x}}{\\sigma} \\frac{x-\\bar{x}}{max(x)-min(x)}\n\\frac{x-min(x)}{max(x)-min(x)} * Lidiar con valores atípicos\nAlternativas:\n\nSacarlos de los modelos o se le puede asignar un cluster\nAlgoritmos de clustering que admitan outliers, de los que vimos cluster difusos (probabilidad) en especifico puede ser DB-SCAN que trabaja por densidad"
  },
  {
    "objectID": "resumen_cluster.html#post-procesamiento",
    "href": "resumen_cluster.html#post-procesamiento",
    "title": "4  Resumen de Cluster",
    "section": "4.4 Post procesamiento",
    "text": "4.4 Post procesamiento\nAlgunos problemas se pueden resolver a través de fusión y división de clusters. Métodos tradicionales generan clústeres esféricos.\nLa fusión de clústeres mas pequeños podría mitigar este problema. División de clústeres mas dispersos también mejora el desempeño\n\n\n\nFusiones y Divisiones"
  },
  {
    "objectID": "resumen_cluster.html#evaluación-de-clusters",
    "href": "resumen_cluster.html#evaluación-de-clusters",
    "title": "4  Resumen de Cluster",
    "section": "4.5 Evaluación de clusters",
    "text": "4.5 Evaluación de clusters\n\nEstadístico de Hopkins {\nInspección visual\nCoeficiente de correlación\nCoeficiente de cohesión\nCoeficiente de separación\nCoeficiente de silueta\n\n\n4.5.1 Estadístico de Hopkins\nAntes de agrupar un conjunto de datos, podemos probar si realmente hay clústeres. Necesitamos probar la hipótesis de la existencia de patrones en los datos contra un conjunto de datos distribuidos uniformemente (distribución homogénea).\n\nEvalúa la tendencia de los cluster\nMide si un conjunto de datos contiene cluster naturales\nUtiliza un test estadístico para la aleatoridad espacial\n\nLa estadística de Hopkins se calcula de la siguiente manera:\n\nMuestrea p puntos a partir del conjunto de datos\nGenerar p puntos aleatorios en el mismo espacio\nCalcula H, donde:\n\nw_i: distancia desde el punto aleatorio i hasta el vecino más cercano en los datos originales\nu_i: distancia desde el punto de muestra i hasta el vecino más cercano en los datos originales\n\n\n\n\n\nFormula de Estadístico de Hopkins\n\n\nLos valores de H cercanos a 0,5 indican datos aleatorios, a 1 indica datos altamente agrupados y a 0 indica una distribución uniforme.\n\n\n\nVisualización Gráfica de Estadístico de Hopkins\n\n\n\n\n4.5.2 Inspección visual\n\nCrear la matriz de distancia\nOrdene la matriz en función de las etiquetas de clúster obtenidas.\nInspeccione visualmente\n\nLas buenas agrupaciones exhiben un patrón de bloque claro con “mismo color”\n\n\n\nInspescción Visual\n\n\n\n\n4.5.3 Coeficiente de correlación\n\nConstruir la matriz de similitud inicial entre todos los puntos s(i,j) = 1 / (1+d(i,j))\nConstruye la matriz de similitud “ideal” basada en la pertenencia al clúster\nCalcule la correlación entre la matriz de similitud inicial y la matriz de similitud “ideal” (los ejes X e Y son la similitud inicial/ideal respectivamente).\nLa alta correlación indica que los puntos del mismo clúster están cerca el uno del otro\n\n\n\n\nCoeficiente de correlación\n\n\n\n\n4.5.4 Coeficiente de cohesión\nMide cuán estrechamente relacionados están los objetos dentro de cada clúster.\n\n\n\nCohesión\n\n\nSuma de errores cuadrados (SSE) es la suma de la distancia cuadrada de un punto al centroide de su clúster.\n\n\n\nFormula de Coeficiente de cohesión\n\n\n\n\n\nEsquema de Coeficiente de cohesión\n\n\n\n\n4.5.5 Coeficiente de separación\nMide cuán distinto es un clúster de los otros clusters.\n\n\n\nCoeficiente de separación\n\n\nLa suma de cuadrados intra grupo (SSB) es la suma de la distancia cuadrada de un centroide de clúster a la media general. Minimizar la cohesión equivale a maximizar la separación.\n\n\n\nFormula de Coeficiente de separación\n\n\n\n\n4.5.6 Coeficiente de silueta\nCombina cohesión y separación. Normalmente varía entre -1 y 1, donde\n\nValores cercanos a 1 implican una mejor agrupación en clústeres.\nValor negativo implica que el punto i está más cerca de otro clúster\n\n\n\n\nGráfico de Coeficiente de silueta\n\n\nPara un punto individual i:\n\nCalcular a_i como la distancia media de i a puntos en el mismo clúster\nCalcule b_{ij} como la distancia media del punto i a todos los puntos del clúster j.\nCalcular b_i como el b_{ij} mínimo excluyendo el clúster propio.\nEl coeficiente de silueta para el punto i es:\n\nS_i =\\frac{(b_i-a_i)}{max(a_i,b_i)}\n\n\n\nTabla de Coeficiente de silueta\n\n\nEl coeficiente de silueta de un clúster es el promedio de los coeficientes de silueta de los puntos pertenecientes."
  },
  {
    "objectID": "resumen_cluster.html#métodos-de-clusterización",
    "href": "resumen_cluster.html#métodos-de-clusterización",
    "title": "4  Resumen de Cluster",
    "section": "4.6 Métodos de Clusterización",
    "text": "4.6 Métodos de Clusterización\n\n4.6.1 Basados en Particiones\n\n\n\nMétodos basados en particiones\n\n\n\n4.6.1.1 K-medias\n\nUno de los algoritmos de clusters más simples.\nDado un número K de clusters (determinado por el usuario), cada cluster está asociado con un centroide y cada entidad se asigna al cluster con el centroide más cercano.\nVariantes como K-medioides, o K-modas usan otros estadísticos como centroides\n\n\n\n\nK-medias\n\n\n\n\n\nAlgoritmo de K-medias\n\n\n\n\n\nSelección de Centroides K-medias\n\n\n\n\n4.6.1.2 Métodos jerárquicos\nLas entidades se agrupan en una jerarquía de clústeres anidados.\n Los métodos jerárquicos se subdividen en aglomerativos y disociativos. Cada una de estas categorías presenta una gran diversidad de variantes. 1. Los métodos aglomerativos, también conocidos como ascendentes, comienzan el análisis con tantos grupos como individuos haya. A partir de estas unidades iniciales se van formando grupos, de forma ascendente, hasta que al final del proceso todos los casos tratados están englobados en un mismo conglomerado. 2. Los métodos disociativos, también llamados descendentes, constituyen el proceso inverso al anterior. Comienzan con un conglomerado que engloba a todos los casos tratados y, a partir de este grupo inicial, a través de sucesivas divisiones, se van formando grupos cada vez más pequeños. Al final del proceso se tienen tantas agrupaciones como casos han sido tratados.\nAlgoritmo\nEl algoritmo básico para clustering aglomerativo es sencillo:\n\nDeje que cada punto de datos sea un clúster\nCalcular la matriz de proximidad (matriz de distancia entre cada clúster)\nRepetir hasta que sólo quede un solo clúster\n\nFusionar los dos clústeres más cercanos\nActualizar la matriz de proximidad\n\n\nEl paso clave es el cálculo de la proximidad de dos clústeres\nDiferentes enfoques para definir la distancia entre clústeres distinguen los diferentes algoritmos\nDendrogramas\nUn dendrograma es una estructura de tipo árbol que muestra el proceso generativo de agrupación en clústeres. El eje X muestra los puntos de datos originales, mientras que el eje Y podría mostrar la distancia entre clústeres.\n\n\n\nTipos de Dendogramas\n\n\nMétodos de Aglomeración\nA continuación vamos a presentar algunas de las estrategias que pueden ser empleadas a la hora de unir los clusters en las diversas etapas o niveles de un procedimiento jerárquico. Ninguno de estos procedimientos proporciona una solución óptima para todos los problemas que se pueden plantear, ya que es posible llegar a distintos resultados según el método elegido.\n\nSingle linkage:\n\nEsta estrategia recibe en la literatura anglosajona el nombre de amalgamamiento simple (single linkage). En este método se considera que la distancia o similitud entre dos clusters viene dada, respectivamente, por la mínima distancia (o máxima similitud) entre sus componentes.\n\nComplete linkage:\n\nEn este método, también conocido como el procedimiento de amalgamamiento completo (complete linkage), se considera que la distancia o similitud entre dos clusters hay que medirla atendiendo a sus elementos más dispares, o sea, la distancia o similitud entre clusters viene dada, respectivamente, por la máxima distancia (o mínima similitud) entre sus componentes.\n\nAverage linkage:\n\nEn esta estrategia la distancia, o similitud, del cluster C_i con el C_j se obtiene como la media aritmética entre la distancia, o similitud, de las componentes de dichos clusters.\n\n\n\n\n\nMétodos de Aglomeración\n\n\nDesventajas\n\nEl algoritmo es demasiado caro O(n3): Hay n pasos, para unir clústeres, y en cada paso calculamos la matriz de proximidad O(n2).\nUna vez que se toma la decisión de combinar dos grupos, no se puede revertir\nNinguna función objetivo se minimiza directamente\n\n\n\n\n4.6.2 Clusters difusos\nCada punto tiene un probabilidad de pertenecer a cada clúster.\n\n\n\nEjemplo de Clusters difusos\n\n\n\n4.6.2.1 Coeficiente de partición difusa (FPC)\nEl coeficiente de partición difuso evalúa la variabilidad de las asignaciones.\nF(U)= \\frac{tr(U*U^T)}{n}\nDonde U es la matriz de pertenencia, * es el operador de multiplicación entre matrices, y tr( ) es la traza de la matriz, es decir, la suma de los valores diagonales.\nEl coeficiente de partición difusa varía entre 0 y 1 donde un valor cercano a 1 implica menor variabilidad en la matriz de pertenencia, que se asocia a una mejor clusterización de los datos.\n\n\n\nValores de Coeficiente de partición difusa y su representación gráfica\n\n\n\n\n4.6.2.2 Gaussian Mixed Models\nUn modelo de mezcla gaussiana (GMM) asume que los datos se generaron a partir de una mezcla de K gaussianos multidimensionales, donde cada componente tiene parámetros: N_k(\\mu_k,\\Sigma_k)\n\nK es definido por el usuario.*\n\n\n\n\nRepresenación Gráfica Gaussian Mixed Models\n\n\nGaussiono Multivariado\n\n\n\nGaussionao Multivariado\n\n\n Algoritmo\n\n\n\nAlgoritmo GMM"
  },
  {
    "objectID": "resumen_cluster.html#referencias",
    "href": "resumen_cluster.html#referencias",
    "title": "4  Resumen de Cluster",
    "section": "4.7 Referencias",
    "text": "4.7 Referencias\nMétodos Jerárquicos de Análisis Cluster."
  },
  {
    "objectID": "res_a_supervisado.html",
    "href": "res_a_supervisado.html",
    "title": "5  Resumen A. Supervisado",
    "section": "",
    "text": "k-NN es un tipo de aprendizaje basado en instancias o aprendizaje perezoso (lazy).\nk-NN almacena los datos de entrenamiento p-dimensionales y retrasa el proceso de aprendizaje hasta que se debe clasificar una nueva instancia.\nPara predecir un nuevo punto, los vecinos k más cercanos se calculan utilizando la distancia\nLa clasificación final se realiza en función de las etiquetas de clase de estos vecinos.\n\n\n\n\nEjemplo de clasificación con k-NN con sus fronteras de decisión\n\n\n\n\n\n\nDeje que x_j sea un nuevo punto p-dimensional sin etiquetar.\nCalcular d(x_i, x_j) para i={1,...,n}\nEncuentra los vecinos k más cercanos de x_j (d(x_i, x_j) se minimiza).\nEtiqueta x_j basada en los k vecinos más cercanos\n\n\n\n\n\n¿Cuál es la mejor relación calidad-precio para k?\n\nPor lo general, se utiliza un valor pequeño, por ejemplo, k<10.\n\n¿Qué medida de distancia d( ) utilizar?\n\nA menudo se utiliza la distancia euclidiana (L2).\n\n¿Cómo puedo etiquetar xj en función de los vecinos k más cercanos?\n\nA menudo se utiliza el voto mayoritario.\n\n\n\n\n\nParámetros del modelo:\n\nk (número de vecinos)\ncualquier parámetro de medida de distancia (por ejemplo, pesos en las entidades)\n\nFortalezas:\n\nModelo sencillo, fácil de implementar\nAprendizaje muy eficiente: O(1)\n\nDebilidades:\n\nInferencia ineficiente: tiempo y espacio O(n)\nMaldición de la dimensionalidad: A medida que aumenta el número de entidades, necesita un aumento exponencial en el tamaño de los datos para asegurarse de que tiene ejemplos cercanos para cualquier dato dado."
  },
  {
    "objectID": "res_a_supervisado.html#clasificador-bayesiano",
    "href": "res_a_supervisado.html#clasificador-bayesiano",
    "title": "5  Resumen A. Supervisado",
    "section": "5.2 Clasificador bayesiano",
    "text": "5.2 Clasificador bayesiano\n\n5.2.1 Naive Bayes\nNaive Bayes aprende una distribución condicional de la probabilidad. Dado un punto de datos x, la salida del modelo es la probabilidad que x pertenezca a una clase específica.\nNaive Bayes utiliza 3 conceptos clave:\n\nProbabilidad condicional\nTeorema bayesiano\nIndependencia condicional\n\n\n\n\n¿Qué tan probable es comprar una computadora a un estudiante de 23 años con un crédito justo y ingresos medios? x=\\{<=30,medium,yes,fair\\}\n\n\n\nTruco:\n\nAsumiremos Ingenuamente independencia condicional de X_1, X_2, ...,X_k dado C (aunque esto no necesariamente sea cierto)\n\n\nP(C|X_1, X_2, ...,X_k)\\propto \\prod_{i=1}^{k}P(X_i|C)P(C)\nAhora fácilmente podemos calcular cada P(X_i|C) a partir de los datos.\n\n\n5.2.2 Algoritmo de Naive Bayes\n\nFase de aprendizaje: Dado un conjunto de entrenamiento S,\n\n\nSalida: Tablas de Probabilidad condicional; para elemento x_j, X_j \\times L\n\nFase de prueba: dada una instancia desconocida X' =(a'_1,...,a'_n)\n\nBuscar tablas para asignar la etiqueta c* a X' si:\n\n\n\n5.2.3 Características de Naive Bayes\nFortalezas:\n\nFácil de implementar y se puede aprender de forma incremental\nA menudo funciona bien incluso cuando se viola la suposición de independencia\nSe puede aprender gradualmente\nLos valores que faltan se ignoran en el proceso de aprendizaje\nModelo robusto con respecto a valores atípicos y datos irrelevantes\n\nDebilidades:\n\nLa suposición condicional de clase produce estimaciones de probabilidad sesgadas\nLas dependencias entre variables no se pueden modelar\n\n\n\n5.2.4 Conclusiones de Naive Bayes\n\nNaive Bayes se basa en el supuesto de la independencia\n\nEl entrenamiento es muy fácil y rápido; solo requiere considerar cada atributo en cada clase por separado\nLa prueba es sencilla; simplemente buscando tablas o calculando probabilidades condicionales con distribuciones normales\n\nUn modelo generativo popular\n\nPerformance es competitivo para la mayoría de los clasificadores de última generación, incluso en presencia de una violación de la suposición de independencia\nMuchas aplicaciones exitosas, por ejemplo, filtrado de correo no deseado"
  },
  {
    "objectID": "res_a_supervisado.html#árboles-de-decisión",
    "href": "res_a_supervisado.html#árboles-de-decisión",
    "title": "5  Resumen A. Supervisado",
    "section": "5.3 Árboles de Decisión",
    "text": "5.3 Árboles de Decisión\n\n5.3.1 Estructura de un Árbol de Decisión\nUn árbol de decisión es una estructura similar a un diagrama de flujo en la que:\n\nCada nodo interno representa una “prueba” en un atributo\nCada rama representa el resultado de la prueba\nCada nodo hoja representa una etiqueta de clase\n\nDado un punto de datos x, la salida del modelo es la probabilidad de que x pertenezca a una clase específica.\n\n\n\nEjemplo de un árbol de decisión\n\n\n\n\n5.3.2 Definición\nUn árbol de decisión segmenta los datos de entrada utilizando secciones rectangulares del espacio\n\n\n\nSegmentación de datos en secciones rectangulares\n\n\n\n\n5.3.3 Algoritmo de Hunt\nSea D_t el conjunto de registros de entrenamiento que llegan a un nodo T\n\nsi D_t contiene registros que pertenecen a la misma clase y_t, entonces t es un nodo hoja etiquetado como y_t\nSi D_t es un conjunto vacío, entonces t es un nodo hoja etiquetado por la clase predeterminada, y_d\nSi D_t contiene registros que pertenecen a más de una clase, utilice una prueba de atributo para dividir los datos en subconjuntos más pequeños. Aplicar el procedimiento de forma recursiva a cada subconjunto.\n\n\n\n\nDefinición de Algoritmo de Hunt\n\n\n\n\n5.3.4 Consideraciones en Árboles de Decisión\nCómo especificar los puntos de corte de atributos\nNúmero de formas de dividir\n\nDivisión de 2 vías\nDivisión multi vías\n\nTipos de atributos\n\nNominal\nOrdinal\nContinuo\n\nParticionamiento de atributo continuo\n\nPartición binaria: Regla única que divide el atributo en dos subconjuntos (X_j > v). Podría ser computacionalmente costoso, porque debe considerar todas las divisiones posibles y encontrar el mejor corte.\n\n\n\n\nPartición binaria de una variable continua\n\n\n\nDecisión de discretización: Formar un atributo categórico ordinal\n\nEstático: discretizar una vez al principio (edad<30, 30<edad<40, 40<edad)\nDinámico: los rangos pueden variar dependiendo de la rama del árbol\n\n\n\n\n\nDiscretización de una variable continua\n\n\nMétodos de crecimiento completo\n\nTodos los ejemplos de un nodo pertenecen a la misma clase\nNo quedan atributos para divisiones posteriores\nNo quedan muestras\n\n¿Qué impacto tiene esto en la calidad de los árboles aprendidos?\n\nLos árboles sobreajustan los datos y la precisión de las pruebas disminuye.\nEl modelo aprende los datos de entrenamiento, pero no se generaliza a los nuevos datos.\nLa poda se utiliza para evitar el sobreajuste.\n\nPoda\nPre poda\n\nAplicar una prueba estadística para decidir si se debe expandir un nodo\nUtilice una medida explícita de complejidad para penalizar los árboles grandes (por ejemplo, longitud mínima de la descripción)\n\nPost poda\n\nUtilice un conjunto de ejemplos para evaluar la utilidad de podar nodos del árbol (después de que el árbol esté completamente crecido)"
  },
  {
    "objectID": "res_a_supervisado.html#máquinas-de-soporte-vectorial",
    "href": "res_a_supervisado.html#máquinas-de-soporte-vectorial",
    "title": "5  Resumen A. Supervisado",
    "section": "5.4 Máquinas de soporte vectorial",
    "text": "5.4 Máquinas de soporte vectorial\n\n5.4.1 Definición Support Vector Machines (SVM)\nSVM es un modelo discriminativo que busca la mejor manera de dividir un espacio, basándose en algunos puntos de este (support vectors), para separarlo en 2 zonas con clases distintas.\nLuego se construye un clasificador sobre esa división.\n\n\n\nDivisión de el espacio usando vectores de soporte\n\n\n\n\n5.4.2 Descripción de SVM\nPara separar el espacio nosotros tenemos que buscar un hiperplano (de una dimensión menor a las dimensiones del espacio). En un espacio 2D, como en el próximo ejemplo, el hiperplano será una línea.\n\n\n\nLinea (hiperplano) que divide el espacio como frontera de decisión\n\n\nCuando llegue un nuevo punto, podemos evaluar a que lado del hiperplano esta. Dependiendo de a que lado este, lo podemos clasificar (como rojo o azul en este caso).\n\nEl problema es que muchos hiperplanos logran dividir el espacio en 2. ¿Cuál será el mejor? ¿Cómo lo logramos encontrar? ¿Qué factores afectan su posición?\n\n\n\n5.4.3 Frontera de Decisión SVM\nA cada posible hiperplano que cumpla las condiciones, le dibujaremos un margen equidistante a ambos lados hasta tocar el primer punto del espacio.\nDiremos que el mejor hiperplano es aquel que tenga el margen mas grande posible. Esto lo podemos denominar el método del “ancho de avenida”.\n Entre el hiperplano representado por la ecuación de la recta de color naranjo tiene un margen mas grande que el rosado; por lo tanto esta dividiendo el espacio de mejor forma. Los “vectores de soporte” son los puntos que tienen contado con los márgenes del hiperplano.\n\n\n5.4.4 Optimización SVM\nEl problema es de minimización de tipo convexo cuadrático, con muchas restricciones lineales de inigualdades. Con p+1 parámetros (p es la dimensionalidad de la data) y n restricciones (una restricción por cada entidad). \nEn este caso podemos usar el método de optimización de Lagrange para maximizar una nueva función y no preocuparnos de las restricciones. Las restricciones serán reemplazadas por multiplicadores de Lagrange y el proceso de aprendizaje será dictado por productos puntos.\n\n\n\nOptimización de Lagrange para mazimizar una nueva función\n\n\nLagrangiano\nDefinición de L:\n\n\n\nDefinición de L\n\n\nConfiguración de gradientes de L respecto a incógnitas (W y b):\n\n\n\nConfiguración de gradientes\n\n\nRestricción de no-negatividad de los multiplicadores de Lagrange:\n\\alpha \\geq 0 \\space \\forall i \\in \\{1,2, .., n\\} \n\n\n5.4.5 Interpretación geométrica SVM\n\n\n\nInterpretación geométrica de Support Vector Machine\n\n\n\n\n5.4.6 Caso no separable SVM\nSVM, como lo hemos visto hasta ahora tiene un primer problema. No siempre se puede dividir el espacio con un hiperplano que cumpla las condiciones:\ny_i(w \\cdot X_i+b)-1 \\geq 0\n\nPara solventar esto, podemos agregar a SVM una “holgura”, que permita soportar una determinada cantidad de puntos en el incorrecto lado de la calle; con el fin de lograr generar el hiperplano.\n\n\n\n\nCaso no separable\n\n\nAl someter la nueva función de optimización, con sus restricciones, al Lagrange, obtenemos un resultado prácticamente igual:\n\n\n\nOptimización para el caso no separable\n\n\nSensibilidad de C\n\n\n\nSensibilidad de C\n\n\n\n\n5.4.7 Limitaciones SVM clásico\nImagina que aplicamos SVM sobre el próximo problema. Puedes seleccionar cualquier C que quieras. Dibuja la mejor línea de separación.\n\n\n\nAunque hay claramente 2 clases distintas, no podemos dividir el espacio con el hiperplano de ninguna manera que nos de un buen resultado.\nEs un problema donde necesitamos dividir el espacio de una manera no lineal.\n\nTruco del Kernel\nSVM normal funciona bien para DataSet que se pueden dividir linealmente (aunque tengan un poco de ruido, para eso usamos C):\n\n¿Pero que hacemos con data set donde no podemos separar linealmente?\n\n“Mapeamos” o transformamos el DataSet a un dimensión mayor, y en ese lugar buscamos si podemos encontrar un hiperplano y aplicar SVM.\n\nNuestro nuevo problema de optimización, considerando una función de Kernel se ve así:\n\nExisten cientos de Kernels, distintos (es un área de estudio académico):\n\nLinear => K(x_i,x_j) = x_i^Tx_j\nPolynomial kernel with degree d => K(x_i,x_j) = (x_i^T x_j+1)^d\nRadial basis function kernel with width \\sigma => K(x_i,x_j) = exp(-||x_i-x_j||^2/(2\\sigma^2))\nSigmoid with parameter κ and θ => K(x_i,x_j) = tanh(κ*x_i^T x_j + θ)\n\n\n\n5.4.8 Debilidades y fortalezas SVM\nDebilidades:\n\nEl proceso de entrenamiento y prueba es lento, debido a tener que solucionar un problema de Lagrange.\nEn casi todas sus variedades solo puede clasificar de manera binaria (+1,-1)\nMuy sensitivo al ruido.\nLo peor: lograr escoger la función de Kernel correcta.\n\nFortalezas:\n\nEl entrenamiento es fácil, la solución es única y global para todo el espacio.\nSVM no sufre de la maldición de la dimensionalidad !\nNo se genera sobretratamiento de manera muy fácil.\nFácil de entender de manera geométrica.\n\n\n\n5.4.9 Modelos Ensamblados\n\n5.4.9.1 Dilema sesgo-varianza\nEl error de predicción de cualquier modelo de machine learning puede ser separado en tres términos:\n\nVarianza/Variance: ¿qué tan factible es que cambia nuestra predicción si usamos otro set de datos?. Se relaciona a la complejidad del modelo.\nRuido/Noise: error base imposible de reducir (variables desconocidas, problemas de adquisición de datos, etc).\nSesgo/Bias: ¿qué tan lejana es nuestra predicción promedio con respecto al verdadero valor? Se relaciona a la suposiciones/simplicidad del modelo.\n\n\n\n\n\nSesgo/varianza\n\n\n\n\n\nRelaciones en un modelo\n\n\n\n\n5.4.9.2 Introducción\nEs muy difícil construir un solo modelo que obtenga un buen rendimiento (bajo sesgo y varianza).\nEnfoque: construyamos múltiples modelos “complejos” o “sencillos” con el mismo o distinto set de datos y combinemos sus predicciones.\nObjetivo: obtener “un modelo final” con bajo error de sesgo y varianza.\n\nModelos complejos => bajo error de sesgo y alto error de varianza. Combinación de predicciones => bajar la varianza de la predicción.\nModelos sencillos => alto error de sesgo y bajo error de varianza. Combinación de predicciones => mantendría el alto sesgo, excepto que los modelos se enfoquen en los puntos mal clasificados.\n\n\n\n\n5.4.9.3 Problema multi-clase\nProblemas con múltiple clases son usualmente complejos. Un problema multi-clase se puede simplificar a múltiples problemas binarios\n\nUn problema con m clases puede ser resuelto con m modelos, calculando P(C_i|M_i), donde Mi es un modelo binario para la clase i.\nUn problema con m clases puede ser resuelto con \\frac{m(m-1)}{2} modelos, donde cada modelo se enfoca solo entre dos clases.\n\nLa predicción final estará dada por:\n\nMayor probabilidad.\nMayoría.\nVoto ponderado.\n\n\n\n\n5.4.9.4 Bootstrap aggregating (Bagging)\n\n\n\n5.4.9.5 Bagging\nDado un set de entrenamiento D={(x1,y1),..., (xN,yN)}, y M modelos\nFor m in range(M):\nObtener una muestra bootstrap (samplear con repetición N puntos) del set de datos D, generando la muestra Dm. Aprender el modelo Mm de Dm\nPara clasificar el punto xt se aplica cada uno de los M modelos (M1,M2,…,Mm) a xt y se considera la clase con más votos o un promedio (ponderado).\nLos modelos tienen errores no correlacionados debido a la diversidad de puntos en cada muestra bootstrap\nEjemplo\n\n\n\n5.4.9.6 Random forest\nUno de los modelos de ensamblados más conocidos es el random forest, el cual es un ensamblado de árboles de decisión.\n\n\nEl proceso de combinación puede ser promedio simple o mayoría.\nCada árbol de decisión será “distinto” (limitando el número de variables) en comparación a un puro árbol de decisión.\nEl uso de múltiples árboles “distintos” y complejos nos permite reducir la componente de la varianza del error.\n\n\n\n5.4.9.7 Boosting\n\nAsignar a cada punto de entrenamiento D={(x_1,y_1),..., (x_N,y_N)}, un peso igual a w_i = 1/N, obteniendo el vector de probabilidades w.\nFor m in range(M):\n\nGenerar Dm usando los pesos w Aprender el modelo Mm de Dm\nCalcular el error de Mm y cambiar los pesos wi de los puntos incorrectamente clasificados\nNormalizar los pesos wi para que sumen 1\nEstimar la importancia del modelo 𝛂m basado en los errores\nPara clasificar el punto xt se aplica cada uno de los M modelos (M1,M2,…,Mm) a xt y se considera un promedio ponderado usando 𝛂m.\n\n\n\n5.4.9.8 Adaboost\nUno de los algoritmos más famosos de boosting es adaboost, el cual es un ensamblado de modelos sencillos, donde:\n\n\nLa predicción final es realizada con un promedio ponderado, basado en el error del modelo.\nCada modelo será sencillo (obteniendo un alto sesgo).\nEl uso de modelos sencillos reduce la varianza del error.\nEl foco en puntos mal clasificados reduce el sesgo del error.\n\n\nEjemplo:\nPrimera Iteración\n\nSegunda Iteración\n\nFinal\n\n\n\n5.4.9.9 Gradient boosting\nGadient boosting, a diferencia de los modelos previos, se enfoca en predecir en forma correcta los errores de los modelos anteriores.\n\n\nLa predicción final corresponde a la suma de todos los modelos.\nCada modelo será sencillo (obteniendo un alto sesgo).\nEl uso de modelos sencillos reduce la varianza del error.\nEl foco en los errores de clasificaciones previas reduce el sesgo del error. Sin embargo, esto puede causar sobreentrenamiento.\n\nAlgoritmo de Gradiente Boosting\n\n\n\n5.4.9.10 Regressor tree\nUn regressor tree es un árbol de decisión enfocado a regresión, donde cada separación busca generar grupos similares de datos.\nEn vez de “gini” o “entropy” el criterio a minimizar es el “squared_error” que corresponde al error cuadrático medio (otro criterios existen).\nEn la práctica esto es equivalente a buscar una separación que minimice la varianza de los datos dentro de cada hoja. La “clasificación” final corresponde al promedio de los datos de cada hoja."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "6  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  }
]